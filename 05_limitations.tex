\section{Conclusion, Limitations \& Future Work}
\label{sec:limitations}
In summary, our method has demonstrated the efficacy of employing a reconstruction combined with a diffusion model to enhance the aesthetic quality of 3D instances in a reconstructed scene. Furthermore, our approach enables a simple process to create virtual environments from single images, which could be utilized in games, and virtual or augmented reality settings. Utilizing a multi-modal diffusion model instead of leveraging high-quality 3D objects directly (such as CADs), leads to another advantage, as the inputs can be customized enabling fine-grained control of the reconstructed scene. 
\\

While our results are promising, we also point out a few limitations to our method. The first one concerns the detection of objects. While the panoptic reconstruction model doesn't necessarily need to detect an instance to reconstruct its approximate shape, SDFusion is only applied to detected objects. Therefore undetected objects would not be refined, or if they are misidentified as a part of another instance, might be lost entirely. These noisy instance segmentations raise another issue as well, as our scale and position estimates rely on the instance panoptic properties. Therefore instance segmentations which include parts of other instances or parts of the wall/floor, lead to larger, misplaced refined instances. 
\\

To improve upon the mentioned limitations we have two directions in which future work could follow up. The first one could implement end-to-end training with adapted losses which would punish misidentified instances more. The second improvement would focus on the merging process. A pose estimation network, which predicts the scale and rotation of the instance might alleviate our issues arising from the usage of the chamfer distance and bounding boxes for these two properties, as both are vulnerable to outliers. Another area of interest could be themed/edited instance reconstructions through descriptions that can be added during inference, as the SDFusion paper shows interesting results based on detailed object descriptions.

% We conducted separate training for both the Panoptic model and the SDFusion model. To increase the performance we advocate for an end-to-end training approach. Given that the Panoptic model outputs occupancies and a distance field in terms of geometry, a differentiable transformation to a signed distance field is necessary to allow end-to-end training. This integration would enable SDFusion to effectively backpropagate gradients and directly learn from SDFusion's noisy inputs. Another constraint lies in our registration algorithm, which selects one out of 16 predefined positions, which can pose a challenge in achieving perfect alignment relying on the initial orientation of the reconstructed objects. A more robust alignment strategy can enhance the overall quality of the final scene.