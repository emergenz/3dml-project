\section{Related Work}
\label{sec:related}

\paragraph{2D panoptic segmentation}
2D panoptic segmentation merges semantic and instance segmentation, providing detailed pixel-level parsing of images, capturing both general categories (semantic segmentation)
and individual object identities (instance segmentation) \citep{kirillov2019panoptic}. Since the original task formulation by \citet{kirillov2019panoptic}, a number of works
have been proposed to solve the task \cite{wang2020axial, cheng2020panoptic, mohan2021efficientps, Li_2021_CVPR, Wang_2021_CVPR, Li_2022_CVPR, Kundu_2022_CVPR, Cheng_2022_CVPR, Yu_2022_CVPR, Xu_2023_CVPR, Chen_2023_ICCV, li2023mask, yu2023convolutions},
while more recent approaches \citep{Jain_2023_CVPR} try to unify image segmentation in its entirety.

\paragraph{Single-view 3D reconstruction}
The work by \citet{snavely2006photo} was the first notable attempt at reconstructing 3D scenes from unordered photo collections. Since then, the field of image-based 3D reconstruction
has seen a number of advancements, culminating in the task of single-view 3D reconstruction \citep{choy20163d, wang2018pixel2mesh, mescheder2019occupancy, huang2019perspectivenet, shin20193d, denninger20203d, nie2020total3dunderstanding}.

\paragraph{Shape priors}
\citet{wu2018learning} note that the task of single-view 3D reconstruction is non-deterministic, as there are many 3D shapes that can explain a given single-view input, and propose
to use shape priors to shape the solution space such that the reconstructed shapes are realistic, but not necessarily the ground truth.

\paragraph{3D scene understanding}
The task of 3D scene understanding and panoptic reconstruction is analogous to its 2D counterpart and aims to infer the 3D structure and semantics of a scene, including the 3D layout, object instances, and their 3D shapes from images \citep{dahnert2021panoptic} or noisy geometry \citep{hou20193d, hou2020revealnet}.
\citet{dahnert2021panoptic} propose a method -- henceforth called \emph{Panoptic 3D} -- to jointly solve the tasks of 3D scene understanding and single-view 3D reconstruction by lifting features produced by a 2D backbone into a 3D volume of the camera frustrum, and jointly optimizing for geometric reconstruction as well as semantic and instance segmentation. 

\paragraph{Modality-conditioned shape generation}
3D generative models represent objects in a variety of modalities, including point clouds \citep{achlioptas2018learning, luo2021diffusion}, occupancy grids \citep{mescheder2019occupancy}, meshes \citep{mo2019structurenet}, and signed distance functions \citep{park2019deepsdf}.
Furthermore, these models can also be distinguished by the type of input they take, such as incomplete shapes \citep{dai2017shape}, images \citep{fan2017point}, text \citep{liu2022towards, zhao2023michelangelo}, or other modalities \citep{Zhou_2021_CVPR}. 
Notably, \citet{cheng2023sdfusion} propose \emph{SDFusion}, a 3D object reconstruction method conditioned on images, text and geometrical input.

\paragraph{Datasets}
Research in 3D panoptic reconstruction relies heavily on datasets to train and evaluate algorithms. Notable datasets in this domain include ScanNet (cite here) and Replica (cite here), which provide rich annotations for scene understanding tasks.

Among these datasets, the 3D Front dataset stands out for its comprehensive coverage of indoor scenes. Created by Li et al. (cite here), 3D Front offers detailed geometric reconstructions, semantic segmentation, and instance segmentation annotations for various indoor environments, including living rooms, kitchens, and bedrooms.
The synthetic 3D dataset contains 6,801 mid-size apartments with 18,797 rooms populated by 3D shapes from the 3D-Future (cite here) dataset. The dataset's high-quality data acquisition process ensures accurate representations, making it a valuable resource for advancing research in 3D panoptic reconstruction. 

