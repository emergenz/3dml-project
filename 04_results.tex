\section{Results}
\label{sec:results}

Our study demonstrates the efficacy of our methodology in enhancing the visual presentation of objects within 3D reconstructions. As depicted in Figure 1, the unprocessed Panoptic reconstruction exhibits visual artifacts and irregularities, while our approach yields smoothed surfaces, facilitating improved recognition through visual observation. Additionally, we observe the sensitivity of our alignment procedure to the instance masks generated by Panoptic. Although the predicted objects maintain smoothness, our alignment algorithm occasionally results in object intersection, as illustrated in \cref{fig:lim}.

\paragraph{Panoptic Reconstruction Training}

We leverage our synthesized dataset to refine the training of the panoptic reconstruction model proposed by \citet{dahnert2021panoptic}.
Initially, we pretrain the 2D encoder, depth estimation and 2D instance prediction with an ADAM optimizer using a batch size of 1 and learning rate 1e-4 for 570k iterations.

The evaluation results for our 2D model compared to the pre-trained model from \citet{dahnert2021panoptic} are presented in \cref{tab:2dresults}.
As illustrated in \cref{fig:qual_panoptic}, our approach shows performance comparable with the pre-trained model. However, it encounters challenges in generating completely clear depth results, occasionally displaying some irregularities.
Despite our efforts, limitations such as time constraints and the relatively small size of our dataset hindered our ability to train a 3D model that achieves performance on par with the pre-trained counterpart. We refer to the future work section in this regard.
\begin{table}
  \centering
  \begin{tabular}{@{}lccc@{}}
    \toprule
     & Depth & Box Class. & Box Regress. \\
    \midrule
    \citet{dahnert2021panoptic} & 0.23 & 3.39 & \textbf{0.092}\\
    Ours & \textbf{0.196} & \textbf{1.3} & 0.149 \\
    \bottomrule
  \end{tabular}
  \caption{Results for joint training of the 2D encoder, depth estimation and 2D instance prediction. For depth we report the $\ell_1$ distance between the predicted and ground-truth depth maps. Additionally we report the $\ell_1$ distance for the regressed 2D boxes and a CE-loss on the box classification.  }
  \label{tab:2dresults}
\end{table}

\begin{figure}[t]
  \centering
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/depth_ours.png}
    \label{subfig:sub1}
   \vspace*{-3mm} % Adjust vertical spacing between the caption and the images
  \caption{Depth map (ours).}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/depth_pan.png}
    \label{subfig:sub2}
   \vspace*{-3mm} % Adjust vertical spacing between the caption and the images
  \caption{Depth map (\citep{dahnert2021panoptic}).}
  \end{subfigure}

  \vspace{0.03\linewidth} % Adjust vertical spacing between rows of figures

  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/depthply_ours.png}
    \label{subfig:sub3}
   \vspace*{-3mm} % Adjust vertical spacing between the caption and the images
   \caption{Geometry from depth (ours).}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.45\linewidth}
    \centering
    \includegraphics[width=\linewidth]{figs/depthply_pan.png}
    \label{subfig:sub4}
   \vspace*{-3mm} % Adjust vertical spacing between the caption and the images
   \caption{Geometry from depth (\citep{dahnert2021panoptic}).}
  \end{subfigure}

  \caption{2D results from the Panoptic 3D model. Our re-training results (left) vs. results from \citet{dahnert2021panoptic} (right).}
  \label{fig:qual_panoptic}
\end{figure}

\paragraph{Refined Reconstruction}

\begin{figure*}%
  \begin{subfigure}[t]{109mm}
    \includegraphics[width=\linewidth]{images/image1.png}
    \caption{While panoptic reconstruction doesn't reconstruct unobserved regions, leading to artifacts and missing geometries, our method generates a complete 3D geometry for each instance}\label{fig:comparison_good}
  \end{subfigure}
  \qquad
  \begin{subfigure}[t]{56mm}
    \includegraphics[width=\linewidth]{images/image2.png}
    \caption{There are also scenarios in which our method has difficulties to reconstruct a scene, which we want to display. On the left you can see a scene in which our method fails because of missing instances, while on the right an object is misclassified, therefore leading to a wrong reconstruction.}\label{fig:comparison_bad}
  \end{subfigure}
  \caption{Comparison of panoptic vs ours}
  \label{fig:comparison_all}
\end{figure*}



Comparing reconstructed scenes from our method to the ones generated by panoptic reconstruction, we want to take a look at figure \ref{fig:comparison_good}. Looking at the panoptic reconstruction results, we see instances that are noisy and do not contain clear edges from other geometries close by. We also notice that unobserved areas, either occluded by other objects or out of sight, are ignored and not reconstructed, leading to incomplete geometries in the reconstructed scene. Our method, on the other hand, improves all three properties. Instances are clean, have clear distinctions from surrounding geometries and are generated as complete objects. We can explain this behavior through SDFusion's diffusion model, which can generate complete shapes from occluded SDF inputs, text or an image alone. On the other hand, we treat each object separately in SDFusion, therefore having distinct object geometries by nature.
We can also see that our merging process leads to a good placement of the refined instances in the scene. 

Looking at figure \ref{fig:comparison_bad}, we want to point out problems 

In figure \ref{fig:comparison_good}, panoptic reconstruction introduces artifacts for unobserved regions, often occluded by other objects in the single RGB image provided. SDFusion on the other hand, actively tries to generate an object based on the latent noise vector provided, providing complete, high-quality instances, similar to the ones seen during training. 

\paragraph{SDFusion Fine-tuning}
In order to align SDFusion to the shape distribution of 3D-Front, we fine-tune the model on a subset of the 3D-Future dataset \citep{fu20213e}, which contains the objects used in the scenes of 3D-Front.
We train the model for 12k steps using the original hyperparameters from \citet{cheng2023sdfusion} but with a batch size of 32 (see Figure \ref{fig:finetune}).

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{figs/sdfusion_finetune_loss_plot.png}
  \caption{Loss curve of our SDFusion fine-tune (smoothed).}
  \label{fig:finetune}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=80mm, scale=1]{images/image4.png}
  \caption{Fine-tuning results for SDFusion}
\end{figure}
